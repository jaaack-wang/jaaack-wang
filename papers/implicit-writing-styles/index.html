<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta name="title" content="Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors">
  <meta name="description" content="Comprehensive evaluation of LLMs' ability to imitate implicit personal writing styles from limited user examples.">
  <meta name="keywords" content="LLM personalization, writing style imitation, authorship, style transfer, NLP">
  <meta name="author" content="Zhengxiang Wang, Nafis Irtiza Tripto, Solha Park, Zhenzhen Li, Jiawei Zhou">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Zhengxiang Wang">
  <meta property="og:title" content="Catch Me If You Can? Not Yet">
  <meta property="og:description" content="LLMs still struggle to imitate implicit writing styles of everyday authors.">
  <meta property="og:url" content="https://www.zhengxiang-wang.me/papers/implicit-writing-styles/">
  <meta property="og:image" content="https://www.zhengxiang-wang.me/images/research/personalized_writing_2025.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Catch Me If You Can? Not Yet">
  <meta name="twitter:description" content="LLMs still struggle to imitate implicit writing styles of everyday authors.">
  <meta name="twitter:image" content="https://www.zhengxiang-wang.me/images/research/personalized_writing_2025.png">

  <meta name="citation_title" content="Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors">
  <meta name="citation_author" content="Wang, Zhengxiang">
  <meta name="citation_author" content="Tripto, Nafis Irtiza">
  <meta name="citation_author" content="Park, Solha">
  <meta name="citation_author" content="Li, Zhenzhen">
  <meta name="citation_author" content="Zhou, Jiawei">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="EMNLP 2025 (Findings)">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2509.14543.pdf">

  <title>Catch Me If You Can? Not Yet</title>

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
</head>
<body>
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="/papers/time-puzzles/" class="work-item">
          <div class="work-info">
            <h5>Measuring Iterative Temporal Reasoning with Time Puzzles</h5>
            <p>Constraint-based date inference benchmark for iterative temporal reasoning.</p>
            <span class="work-venue">Preprint 2026</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/lvlms-overhearing/" class="work-item">
          <div class="work-info">
            <h5>LVLMs are Bad at Overhearing Human Referential Communication</h5>
            <p>Evaluation of LVLM limitations in overhearing settings.</p>
            <span class="work-venue">EMNLP 2025 Main</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/multi-dimensional-writing-assessments/" class="work-item">
          <div class="work-info">
            <h5>LLMs can Perform Multi-Dimensional Analytic Writing Assessments</h5>
            <p>A case study of L2 graduate-level academic English writing.</p>
            <span class="work-venue">ACL 2025 Main</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/multi-problem-eval-llm/" class="work-item">
          <div class="work-info">
            <h5>Evaluating LLMs with Multiple Problems at once</h5>
            <p>A comprehensive benchmark for zero-shot multi-problem evaluation.</p>
            <span class="work-venue">GEM 2025</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/clustering-document-parts/" class="work-item">
          <div class="work-info">
            <h5>Clustering Document Parts</h5>
            <p>Detecting and characterizing influence campaigns from document collections.</p>
            <span class="work-venue">NLP+CSS 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/rnn-seq2seq-learning/" class="work-item">
          <div class="work-info">
            <h5>Learning Transductions and Alignments with RNN Seq2seq Models</h5>
            <p>Controlled study of transduction learning and generalization limits.</p>
            <span class="work-venue">ICGI 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>

      </div>
    </div>
  </div>

  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block"><a href="https://www.zhengxiang-wang.me/" target="_blank" rel="noopener">Zhengxiang Wang</a>*,</span>
                <span class="author-block"><a href="https://sites.google.com/view/nafis-irtiza-tripto/home" target="_blank" rel="noopener">Nafis Irtiza Tripto</a>*,</span>
                <span class="author-block">Solha Park,</span>
                <span class="author-block">Zhenzhen Li,</span>
                <span class="author-block"><a href="https://joezhouai.com/" target="_blank" rel="noopener">Jiawei Zhou</a></span>
              </div>
              <div class="is-size-5 publication-authors">
                <span class="author-block">EMNLP 2025 (Findings)</span>
              </div>
              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2509.14543.pdf" target="_blank" rel="noopener"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fas fa-file-pdf"></i></span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/jaaack-wang/llms-implicit-writing-styles-imitation" target="_blank" rel="noopener"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fab fa-github"></i></span>
                      <span>Code</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="#" onclick="toggleBibTeX(event)"
                      class="external-link button is-normal is-rounded is-dark bibtex-toggle-btn"
                      aria-controls="BibTeX" aria-expanded="false">
                      <span class="icon"><i class="fas fa-quote-right"></i></span>
                      <span>BibTeX</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                As large language models (LLMs) become increasingly integrated into personal writing tools, a
                critical question arises: can LLMs faithfully imitate an individual's writing style from just a
                few examples? Personal style is often subtle and implicit, making it difficult to specify through
                prompts yet essential for user-aligned generation. This work presents a comprehensive evaluation
                of state-of-the-art LLMs' ability to mimic personal writing styles via in-context learning from a
                small number of user-authored samples. We introduce an ensemble of complementary metrics including
                authorship attribution, authorship verification, style matching, and AI detection to robustly assess
                style imitation. Our evaluation spans over 40,000 generations per model across domains such as news,
                email, forums, and blogs, covering writing samples from more than 400 real-world authors. Results
                show that while LLMs can approximate user styles in structured formats like news and email, they
                struggle with nuanced, informal writing in blogs and forums. Further analysis on prompting strategies
                such as number of demonstrations reveals key limitations in effective personalization. Our findings
                highlight a fundamental gap in personalized LLM adaptation and the need for improved techniques to
                support implicit, style-consistent generation. To aid future research and reproducibility, we
                open-source our data and code.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3 has-text-centered">Main Results</h2>
          <div class="results-carousel">
            <div class="item">
              <div class="content has-text-justified">
                <p>
                  Across authors, few-shot prompting consistently improves authorship attribution over zero-shot
                  prompting, but performance still varies by dataset and author, indicating that implicit personal
                  style imitation remains uneven and domain-sensitive.
                </p>
              </div>
              <img src="static/images/AA_plot.png" alt="Per-author authorship attribution accuracy distributions under few-shot and zero-shot settings." loading="lazy">
              <h2 class="subtitle has-text-centered">Distribution of per-author AA accuracy averaged across all LLMs. Few-shot prompting achieves higher per-author accuracy than zero-shot.</h2>
            </div>
            <div class="item">
              <div class="content has-text-justified">
                <p>
                  Style-model analysis complements AA/AV results: generated texts move closer to target author
                  style distributions under few-shot prompting, but the remaining distance, especially on informal
                  domains, reflects a persistent personalization gap.
                </p>
              </div>
              <div class="columns is-multiline">
                <div class="column is-half">
                  <img src="static/images/style_ccat50.png" alt="Mahalanobis style distance distribution for CCAT50 dataset." loading="lazy">
                  <p class="has-text-centered">CCAT50</p>
                </div>
                <div class="column is-half">
                  <img src="static/images/style_blog.png" alt="Mahalanobis style distance distribution for Blog dataset." loading="lazy">
                  <p class="has-text-centered">Blog</p>
                </div>
              </div>
              <h2 class="subtitle has-text-centered">Distribution of average Mahalanobis distances to target author style models on representative datasets (lower is better).</h2>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Conclusion</h2>
            <div class="content has-text-justified">
              <p>
                This paper presents a comprehensive evaluation of state-of-the-art LLMs on their ability to mimic
                the implicit writing styles of everyday users through few-shot in-context learning. By combining
                authorship attribution, verification, stylometric modeling, and AI generation detection across
                four diverse datasets, we provide strong empirical evidence that despite improvements from
                exemplar-based prompting, current LLMs still struggle to reproduce nuanced personal styles,
                especially in informal and stylistically diverse domains. Our analysis further shows that prompt
                design choices, such as length alignment and content similarity, moderately affect stylistic
                fidelity but do not close the personalization gap. These findings highlight fundamental limitations
                in the stylistic adaptability of LLMs and suggest that achieving truly personalized generation
                remains an open challenge. Future work should explore richer personalization signals and hybrid
                prompting and/or finetuning strategies to better capture the subtleties of individual writing
                styles in real-world settings.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="BibTeX" hidden data-bib-src="static/bibtex/citation.bib">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code></code></pre>
      </div>
    </section>
  </main>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank" rel="noopener">Academic Project Page Template</a>,
              adopted from the <a href="https://nerfies.github.io" target="_blank" rel="noopener">Nerfies</a> project page.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>
</html>
