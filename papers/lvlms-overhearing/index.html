<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta name="title" content="LVLMs are Bad at Overhearing Human Referential Communication - Zhengxiang Wang et al.">
  <meta name="description" content="Evaluation of large vision language models as overhearers in spontaneous human referential communication.">
  <meta name="keywords" content="LVLM, referential communication, overhearing, multimodal reasoning, NLP">
  <meta name="author" content="Zhengxiang Wang, Weiling Li, Panagiotis Kaliosis, Owen Rambow, Susan E. Brennan">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Zhengxiang Wang">
  <meta property="og:title" content="LVLMs are Bad at Overhearing Human Referential Communication">
  <meta property="og:description" content="Evaluating LVLMs on overheard human referential communication.">
  <meta property="og:url" content="https://www.zhengxiang-wang.me/papers/lvlms-overhearing/">
  <meta property="og:image" content="https://www.zhengxiang-wang.me/images/research/lvlm_overhearing.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="LVLMs are Bad at Overhearing Human Referential Communication">
  <meta name="twitter:description" content="Evaluating LVLMs on overheard human referential communication.">
  <meta name="twitter:image" content="https://www.zhengxiang-wang.me/images/research/lvlm_overhearing.png">

  <meta name="citation_title" content="LVLMs are Bad at Overhearing Human Referential Communication">
  <meta name="citation_author" content="Wang, Zhengxiang">
  <meta name="citation_author" content="Li, Weiling">
  <meta name="citation_author" content="Kaliosis, Panagiotis">
  <meta name="citation_author" content="Rambow, Owen">
  <meta name="citation_author" content="Brennan, Susan E.">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="EMNLP 2025 (Main)">
  <meta name="citation_pdf_url" content="https://aclanthology.org/2025.emnlp-main.849.pdf">

  <title>LVLMs are Bad at Overhearing Human Referential Communication</title>

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
</head>
<body>
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="/papers/time-puzzles/" class="work-item">
          <div class="work-info">
            <h5>Measuring Iterative Temporal Reasoning with Time Puzzles</h5>
            <p>Constraint-based date inference benchmark for iterative temporal reasoning.</p>
            <span class="work-venue">Preprint 2026</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/implicit-writing-styles/" class="work-item">
          <div class="work-info">
            <h5>Catch Me If You Can? Not Yet</h5>
            <p>LLMs still struggle to imitate implicit writing styles of everyday authors.</p>
            <span class="work-venue">EMNLP 2025 Findings</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/multi-dimensional-writing-assessments/" class="work-item">
          <div class="work-info">
            <h5>LLMs can Perform Multi-Dimensional Analytic Writing Assessments</h5>
            <p>A case study of L2 graduate-level academic English writing.</p>
            <span class="work-venue">ACL 2025 Main</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">LVLMs are Bad at Overhearing Human Referential Communication</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block"><a href="https://www.zhengxiang-wang.me/" target="_blank" rel="noopener">Zhengxiang Wang</a>,</span>
                <span class="author-block">Weiling Li,</span>
                <span class="author-block"><a href="https://pkaliosis.github.io/" target="_blank" rel="noopener">Panagiotis Kaliosis</a>,</span>
                <span class="author-block"><a href="https://owenrambow.com/" target="_blank" rel="noopener">Owen Rambow</a>,</span>
                <span class="author-block"><a href="https://www.psychology.sunysb.edu/sbrennan-/" target="_blank" rel="noopener">Susan E. Brennan</a></span>
              </div>
              <div class="is-size-5 publication-authors">
                <span class="author-block">EMNLP 2025 (Main)</span>
              </div>
              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://aclanthology.org/2025.emnlp-main.849.pdf" target="_blank" rel="noopener"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fas fa-file-pdf"></i></span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/jaaack-wang/lvlms-overhearing" target="_blank" rel="noopener"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fab fa-github"></i></span>
                      <span>Code</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                During conversation, speakers collaborate on spontaneous referring expressions, which they
                can then re-use in subsequent conversation with the same partner. Understanding such referring
                expressions is an important ability for an embodied agent so that it can carry out tasks in
                the real world. This requires integrating and understanding language, vision, and conversational
                interaction. We study the capabilities of seven state-of-the-art Large Vision Language Models
                (LVLMs) as overhearers to a corpus of spontaneous conversations between pairs of human discourse
                participants engaged in a collaborative object-matching task. We find that such a task remains
                challenging for current LVLMs, which fail to show a consistent performance improvement as they
                overhear more conversations from the same discourse participants repeating the same task for
                multiple rounds. We release our corpus and code for reproducibility and to facilitate future
                research.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3 has-text-centered">Main Results</h2>
          <div class="results-carousel">
            <div class="item">
              <div class="content has-text-justified">
                <p>
                  The main result is that overhearer matching remains challenging for current LVLMs. Accuracy
                  trends across rounds show that state-of-the-art models still fail to achieve robust, consistent
                  improvements as they overhear repeated interactions.
                </p>
              </div>
              <img src="static/images/main_results.png" alt="Average accuracy of LVLMs in the overhearer task over rounds." loading="lazy">
              <h2 class="subtitle has-text-centered">Average accuracy of various LVLMs in the overhearer task over rounds. Shaded areas and error bars denote 95% confidence intervals.</h2>
            </div>
            <div class="item">
              <div class="content has-text-justified">
                <p>
                  Robustness analysis further shows substantial variance across human pairs and object orderings.
                  Even top-performing models show unstable outcomes, highlighting sensitivity to interaction context
                  and indicating limited reliability in overhearing settings.
                </p>
              </div>
              <img src="static/images/R1_30_runs_accu_boxplot.png" alt="Round 1 accuracy boxplots for two best-performing LVLMs across human pairs." loading="lazy">
              <h2 class="subtitle has-text-centered">Round 1 accuracy boxplots of two best-performing LVLMs across 10 human pairs. Each boxplot summarizes 30 runs with different object orderings.</h2>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Conclusion</h2>
            <div class="content has-text-justified">
              <p>
                Our findings demonstrate that modern LVLMs still struggle to resolve referring expressions to
                real-world objects produced during spontaneous conversation, a task that humans excel at when
                they can ground meanings together. Overhearers, whether human or LVLM, perform more poorly in
                a matching task than human addressees, even when they are present to every word of a conversation.
                LVLMs in the overhearer role, even state-of-the-art models, fail to exploit the dynamic nature
                of conversation and do not improve over repeated referring, unlike human overhearers. These
                limitations constrain the practical utility of LVLMs as embodied agents, while also highlighting
                clear directions for future improvement. Given that our primary goal is to benchmark current LVLM
                capabilities in this novel overhearing setting, providing mechanistic insights or finding pathways
                to solutions is beyond the scope of our paper and left to future studies. We release our corpus
                for reproducibility and to support continued research in this area.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@inproceedings{wang-etal-2025-lvlms,
    title = "{LVLM}s are Bad at Overhearing Human Referential Communication",
    author = "Wang, Zhengxiang  and
      Li, Weiling  and
      Kaliosis, Panagiotis  and
      Rambow, Owen  and
      Brennan, Susan",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.849/",
    doi = "10.18653/v1/2025.emnlp-main.849",
    pages = "16758--16782",
    ISBN = "979-8-89176-332-6"
}</code></pre>
      </div>
    </section>
  </main>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank" rel="noopener">Academic Project Page Template</a>,
              adopted from the <a href="https://nerfies.github.io" target="_blank" rel="noopener">Nerfies</a> project page.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>
</html>
