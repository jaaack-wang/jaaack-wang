<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta name="title" content="Learning Transductions and Alignments with RNN Seq2seq Models - Zhengxiang Wang">
  <meta name="description" content="Controlled analysis of RNN seq2seq models on formal transduction tasks, with and without attention.">
  <meta name="keywords" content="RNN seq2seq, transductions, alignment learning, out-of-distribution generalization, formal language theory">
  <meta name="author" content="Zhengxiang Wang">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Zhengxiang Wang">
  <meta property="og:title" content="Learning Transductions and Alignments with RNN Seq2seq Models">
  <meta property="og:description" content="RNN seq2seq learning dynamics across identity, reversal, reduplication, and quadratic copying.">
  <meta property="og:url" content="https://www.zhengxiang-wang.me/papers/rnn-seq2seq-learning/">
  <meta property="og:image" content="https://www.zhengxiang-wang.me/images/research/icgi2023.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Learning Transductions and Alignments with RNN Seq2seq Models">
  <meta name="twitter:description" content="RNN seq2seq learning dynamics across identity, reversal, reduplication, and quadratic copying.">
  <meta name="twitter:image" content="https://www.zhengxiang-wang.me/images/research/icgi2023.png">

  <meta name="citation_title" content="Learning Transductions and Alignments with RNN Seq2seq Models">
  <meta name="citation_author" content="Wang, Zhengxiang">
  <meta name="citation_publication_date" content="2023">
  <meta name="citation_conference_title" content="ICGI 2023 (PMLR 217)">
  <meta name="citation_pdf_url" content="https://proceedings.mlr.press/v217/wang23a/wang23a.pdf">

  <title>Learning Transductions and Alignments with RNN Seq2seq Models</title>

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
</head>
<body>
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="/papers/time-puzzles/" class="work-item">
          <div class="work-info">
            <h5>Measuring Iterative Temporal Reasoning with Time Puzzles</h5>
            <p>Constraint-based date inference benchmark for iterative temporal reasoning.</p>
            <span class="work-venue">Preprint 2026</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/lvlms-overhearing/" class="work-item">
          <div class="work-info">
            <h5>LVLMs are Bad at Overhearing Human Referential Communication</h5>
            <p>Evaluation of LVLM limitations in overhearing settings.</p>
            <span class="work-venue">EMNLP 2025 Main</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/implicit-writing-styles/" class="work-item">
          <div class="work-info">
            <h5>Catch Me If You Can? Not Yet</h5>
            <p>LLMs still struggle to imitate implicit writing styles of everyday authors.</p>
            <span class="work-venue">EMNLP 2025 Findings</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/multi-dimensional-writing-assessments/" class="work-item">
          <div class="work-info">
            <h5>LLMs can Perform Multi-Dimensional Analytic Writing Assessments</h5>
            <p>A case study of L2 graduate-level academic English writing.</p>
            <span class="work-venue">ACL 2025 Main</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/multi-problem-eval-llm/" class="work-item">
          <div class="work-info">
            <h5>Evaluating LLMs with Multiple Problems at once</h5>
            <p>A comprehensive benchmark for zero-shot multi-problem evaluation.</p>
            <span class="work-venue">GEM 2025</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/clustering-document-parts/" class="work-item">
          <div class="work-info">
            <h5>Clustering Document Parts</h5>
            <p>Detecting and characterizing influence campaigns from document collections.</p>
            <span class="work-venue">NLP+CSS 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>

      </div>
    </div>
  </div>

  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">Learning Transductions and Alignments with RNN Seq2seq Models</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block"><a href="https://www.zhengxiang-wang.me/" target="_blank" rel="noopener">Zhengxiang Wang</a></span>
              </div>
              <div class="is-size-5 publication-authors">
                <span class="author-block">ICGI 2023 (PMLR 217)</span>
              </div>
              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://proceedings.mlr.press/v217/wang23a/wang23a.pdf" target="_blank" rel="noopener"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fas fa-file-pdf"></i></span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/jaaack-wang/rnn-seq2seq-learning" target="_blank" rel="noopener"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fab fa-github"></i></span>
                      <span>Code</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="/materials/ICGI2023Slides.pdf" target="_blank" rel="noopener"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fa fa-file-powerpoint"></i></span>
                      <span>Slides</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                The paper studies the capabilities of Recurrent-Neural-Network sequence to sequence
                (RNN seq2seq) models in learning four transduction tasks: identity, reversal, total
                reduplication, and quadratic copying. These transductions are traditionally well studied
                under finite state transducers and attributed with increasing complexity. We find that
                RNN seq2seq models are only able to approximate a mapping that fits the training or
                in-distribution data, instead of learning the underlying functions. Although attention
                makes learning more efficient and robust, it does not overcome the out-of-distribution
                generalization limitation. We establish a novel complexity hierarchy for learning the
                four tasks for attention-less RNN seq2seq models, which may be understood in terms of
                the complexity hierarchy of formal languages, instead of string transductions. RNN
                variants also play a role in the results. In particular, we show that Simple RNN
                seq2seq models cannot count the input length.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3 has-text-centered">Main Results</h2>
          <div class="results-carousel">
            <div class="item">
              <div class="content has-text-justified">
                <p>
                  Across tasks, in-distribution test performance is high while out-of-distribution
                  generalization remains limited. In aggregate full-sequence accuracy, the best attentional
                  model reaches 93.35% on test but only 27.34% on gen, while attention-less models peak at
                  65.91% on test and 8.85% on gen. This gap appears consistently across input lengths.
                </p>
              </div>
              <img src="static/images/full_sequence_accuracy.png" alt="Full-sequence accuracy by input length across tasks, model variants, and attention settings." loading="lazy">
              <h2 class="subtitle has-text-centered">Test/gen full-sequence accuracy per input length across identity, reversal, total reduplication, and quadratic copying.</h2>
            </div>
            <div class="item">
              <div class="content has-text-justified">
                <p>
                  Follow-up experiments confirm that attention improves efficiency but does not solve OOD
                  length generalization. Even with reduced hidden sizes, attention preserves near-perfect
                  in-distribution behavior on easier tasks, while generalization on unseen lengths still
                  degrades and quadratic copying remains the most difficult transduction.
                </p>
              </div>
              <img src="static/images/follow_up_accuracy.png" alt="Follow-up accuracy trends with reduced hidden sizes for attentional seq2seq models." loading="lazy">
              <h2 class="subtitle has-text-centered">Follow-up results for attentional models with smaller hidden sizes on identity, reversal, and total reduplication.</h2>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Conclusion</h2>
            <div class="content has-text-justified">
              <p>
                This study investigated how well the three major types of RNN seq2seq models, with and
                without attention, learn four transduction tasks that require learning varying alignments
                or dependencies between the input and target sequences. Through unified training/evaluation
                conditions and comprehensive experiments, we compared the learning results across tasks,
                different model configurations, and test/gen set performance etc., and highlighted factors
                that influence the learning capabilities and generalization capacity of RNN seq2seq models.
                Unlike previous research, the input alphabet Sigma for our experiments has 26 unique symbols,
                instead of binary, making our results more relevant to real-world tasks that concern, say,
                morpho-phonological transductions. The major findings are further discussed below.
              </p>
              <p>
                <strong>Generalization abilities.</strong> For our experiments, RNN seq2seq models, regardless
                of attention, tend to approximate the training or in-distribution data, instead of learning the
                underlying functions. This makes their out-of-distribution generalization abilities restricted,
                which, however, may not be surprising. For an auto-regressive model like RNN seq2seq models to
                learn these transductions with bounded precision or parameter size, the probability of the correct
                output decreases exponentially as a function of the output length n, i.e., P(target) = (1-epsilon)^n,
                where epsilon is the expected error rate. As n increases indefinitely, the probability of correct
                generation eventually becomes infinitesimal and indistinguishably small. It is true for both
                training and testing where strings of (unseen) longer lengths are generally more difficult to
                fit and generalize to.
              </p>
              <p>
                <strong>Attention.</strong> Attention greatly improves the learning efficiency, by reducing both
                model complexity and sample complexity, as well as the learning robustness, in terms of the
                generalization performance and overfitting problem. However, attention does not overcome the
                out-of-distribution generalization limitation, since it does not change the auto-regressive
                nature of the models. Nonetheless, the impressive learning efficiency accelerated by attention
                echoes its original motivation, i.e., "learning to align".
              </p>
              <p>
                <strong>Task complexity.</strong> For the four tasks: identity (f_A), reversal (f_B), total
                reduplication (f_C), quadratic copying (f_D), we established the following task complexity
                hierarchy for attention-less RNN seq2seq models: f_D > f_C > f_A > f_B. This differs from
                the traditional FST-theoretic viewpoint, which sees f_B strictly more complex than f_A.
                As discussed in the paper, f_B is easier than f_A for RNN seq2seq models because learning
                f_B contains many initially shorter input-target dependencies, instead of constantly-distanced
                dependencies for f_A. This makes iteratively optimizing the model parameters easier with
                backpropagation and results in less complexity for f_B. However, for attentional models, we
                find that f_D > f_C > f_B > f_A, which appears to align with the FST characterizations but
                contradicts with our expectation that f_A and f_B are comparably complex with attention.
              </p>
              <p>
                <strong>RNN variants.</strong> The effect of RNN variants on the seq2seq models is a complicated
                one and interacts with other factors, e.g., attention and the task to learn. When attention is
                not used, GRU and LSTM are generally more expressive than SRNN. The only exception is reversal,
                which SRNN consistently outperforms GRU and LSTM in the test/gen set, regardless of attention.
                Please note that, this exception is only true when SRNN seq2seq models fit the related train set
                with (nearly) 100% full-sequence accuracy. Moreover, for quadratic copying and regardless of
                attention, both GRU and LSTM can count the input length while SRNN cannot, which arguably is not
                a matter of model parameter size.
              </p>
              <p>
                Besides some unexplained puzzles brought up in the paper, good continuations of the current
                research may include experimenting with (1) other types of seq2seq models, such as other
                configuration of RNN seq2seq (e.g., bidirectional encoder), CNN seq2seq and transformer;
                (2) Tape-RNN, which show promising generalization results in various transduction tasks;
                and (3) other novel transduction tasks that specifically test the predictions of the
                complexity hierarchy discussed in the paper. We hope that our study encourages more works
                at the intersection of neural networks and formal language theory in the future.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@InProceedings{pmlr-v217-wang23a,
  title = 	 {Learning Transductions and Alignments with RNN Seq2seq Models},
  author =       {Wang, Zhengxiang},
  booktitle = 	 {Proceedings of 16th edition of the International Conference on Grammatical Inference},
  pages = 	 {223--249},
  year = 	 {2023},
  editor = 	 {Coste, Fran\c{c}ois and Ouardi, Faissal and Rabusseau, Guillaume},
  volume = 	 {217},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--13 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v217/wang23a/wang23a.pdf},
  url = 	 {https://proceedings.mlr.press/v217/wang23a.html}
}</code></pre>
      </div>
    </section>
  </main>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank" rel="noopener">Academic Project Page Template</a>,
              adopted from the <a href="https://nerfies.github.io" target="_blank" rel="noopener">Nerfies</a> project page.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>
</html>
