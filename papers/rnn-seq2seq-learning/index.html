<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta name="title" content="Learning Transductions and Alignments with RNN Seq2seq Models - Zhengxiang Wang">
  <meta name="description" content="Controlled analysis of RNN seq2seq models on formal transduction tasks, with and without attention.">
  <meta name="keywords" content="RNN seq2seq, transductions, alignment learning, out-of-distribution generalization, formal language theory">
  <meta name="author" content="Zhengxiang Wang">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Zhengxiang Wang">
  <meta property="og:title" content="Learning Transductions and Alignments with RNN Seq2seq Models">
  <meta property="og:description" content="RNN seq2seq learning dynamics across identity, reversal, reduplication, and quadratic copying.">
  <meta property="og:url" content="https://www.zhengxiang-wang.me/papers/rnn-seq2seq-learning/">
  <meta property="og:image" content="https://www.zhengxiang-wang.me/images/research/icgi2023.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Learning Transductions and Alignments with RNN Seq2seq Models">
  <meta name="twitter:description" content="RNN seq2seq learning dynamics across identity, reversal, reduplication, and quadratic copying.">
  <meta name="twitter:image" content="https://www.zhengxiang-wang.me/images/research/icgi2023.png">

  <meta name="citation_title" content="Learning Transductions and Alignments with RNN Seq2seq Models">
  <meta name="citation_author" content="Wang, Zhengxiang">
  <meta name="citation_publication_date" content="2023">
  <meta name="citation_conference_title" content="ICGI 2023 (PMLR 217)">
  <meta name="citation_pdf_url" content="https://proceedings.mlr.press/v217/wang23a/wang23a.pdf">

  <title>Learning Transductions and Alignments with RNN Seq2seq Models</title>

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
</head>
<body>
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="/papers/time-puzzles/" class="work-item">
          <div class="work-info">
            <h5>Measuring Iterative Temporal Reasoning with Time Puzzles</h5>
            <p>Constraint-based date inference benchmark for iterative temporal reasoning.</p>
            <span class="work-venue">Preprint 2026</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/lvlms-overhearing/" class="work-item">
          <div class="work-info">
            <h5>LVLMs are Bad at Overhearing Human Referential Communication</h5>
            <p>Evaluation of LVLM limitations in overhearing settings.</p>
            <span class="work-venue">EMNLP 2025 Main</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/implicit-writing-styles/" class="work-item">
          <div class="work-info">
            <h5>Catch Me If You Can? Not Yet</h5>
            <p>LLMs still struggle to imitate implicit writing styles of everyday authors.</p>
            <span class="work-venue">EMNLP 2025 Findings</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/multi-dimensional-writing-assessments/" class="work-item">
          <div class="work-info">
            <h5>LLMs can Perform Multi-Dimensional Analytic Writing Assessments</h5>
            <p>A case study of L2 graduate-level academic English writing.</p>
            <span class="work-venue">ACL 2025 Main</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/multi-problem-eval-llm/" class="work-item">
          <div class="work-info">
            <h5>Evaluating LLMs with Multiple Problems at once</h5>
            <p>A comprehensive benchmark for zero-shot multi-problem evaluation.</p>
            <span class="work-venue">GEM 2025</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/clustering-document-parts/" class="work-item">
          <div class="work-info">
            <h5>Clustering Document Parts</h5>
            <p>Detecting and characterizing influence campaigns from document collections.</p>
            <span class="work-venue">NLP+CSS 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>

      </div>
    </div>
  </div>

  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">Learning Transductions and Alignments with RNN Seq2seq Models</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block"><a href="https://www.zhengxiang-wang.me/" target="_blank" rel="noopener">Zhengxiang Wang</a></span>
              </div>
              <div class="is-size-5 publication-authors">
                <span class="author-block">ICGI 2023 (PMLR 217)</span>
              </div>
              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://proceedings.mlr.press/v217/wang23a/wang23a.pdf" target="_blank" rel="noopener"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fas fa-file-pdf"></i></span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/jaaack-wang/rnn-seq2seq-learning" target="_blank" rel="noopener"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fab fa-github"></i></span>
                      <span>Code</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                This paper studies how RNN seq2seq models learn four classic string transductions:
                identity, reversal, total reduplication, and quadratic copying. Under controlled training
                and evaluation settings, it compares SRNN, GRU, and LSTM architectures with and without
                attention on both in-distribution and out-of-distribution sequence lengths. The study
                shows that models often fit in-distribution data well but struggle to learn truly
                length-general functions, and it characterizes how attention and architecture choice
                change learning efficiency and task difficulty.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3 has-text-centered">Main Results</h2>
          <div class="results-carousel">
            <div class="item">
              <div class="content has-text-justified">
                <p>
                  Across tasks, in-distribution test performance is high while out-of-distribution
                  generalization remains limited. In aggregate full-sequence accuracy, the best attentional
                  model reaches 93.35% on test but only 27.34% on gen, while attention-less models peak at
                  65.91% on test and 8.85% on gen. This gap appears consistently across input lengths.
                </p>
              </div>
              <img src="static/images/full_sequence_accuracy.png" alt="Full-sequence accuracy by input length across tasks, model variants, and attention settings." loading="lazy">
              <h2 class="subtitle has-text-centered">Test/gen full-sequence accuracy per input length across identity, reversal, total reduplication, and quadratic copying.</h2>
            </div>
            <div class="item">
              <div class="content has-text-justified">
                <p>
                  Follow-up experiments confirm that attention improves efficiency but does not solve OOD
                  length generalization. Even with reduced hidden sizes, attention preserves near-perfect
                  in-distribution behavior on easier tasks, while generalization on unseen lengths still
                  degrades and quadratic copying remains the most difficult transduction.
                </p>
              </div>
              <img src="static/images/follow_up_accuracy.png" alt="Follow-up accuracy trends with reduced hidden sizes for attentional seq2seq models." loading="lazy">
              <h2 class="subtitle has-text-centered">Follow-up results for attentional models with smaller hidden sizes on identity, reversal, and total reduplication.</h2>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Conclusion</h2>
            <div class="content has-text-justified">
              <p>
                RNN seq2seq models primarily learn mappings that fit seen lengths rather than the
                underlying transduction functions, yielding weak out-of-distribution generalization.
                Attention improves convergence speed, sample efficiency, and robustness, but does not remove
                this core limitation. The experiments also support a task-complexity hierarchy for
                attention-less models and show architecture-specific behaviors, including SRNN difficulty
                with counting in quadratic copying.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@InProceedings{pmlr-v217-wang23a,
  title = 	 {Learning Transductions and Alignments with RNN Seq2seq Models},
  author =       {Wang, Zhengxiang},
  booktitle = 	 {Proceedings of 16th edition of the International Conference on Grammatical Inference},
  pages = 	 {223--249},
  year = 	 {2023},
  editor = 	 {Coste, Fran\c{c}ois and Ouardi, Faissal and Rabusseau, Guillaume},
  volume = 	 {217},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--13 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v217/wang23a/wang23a.pdf},
  url = 	 {https://proceedings.mlr.press/v217/wang23a.html}
}</code></pre>
      </div>
    </section>
  </main>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank" rel="noopener">Academic Project Page Template</a>,
              adopted from the <a href="https://nerfies.github.io" target="_blank" rel="noopener">Nerfies</a> project page.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>
</html>
