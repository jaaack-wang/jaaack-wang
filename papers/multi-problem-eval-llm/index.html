<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta name="title" content="Evaluating LLMs with Multiple Problems at once - Zhengxiang Wang, Jordan Kodner, Owen Rambow">
  <meta name="description" content="A systematic zero-shot benchmark showing when LLMs can and cannot solve multiple problems in one prompt.">
  <meta name="keywords" content="LLM evaluation, multi-problem prompting, zero-shot, reasoning, benchmarking">
  <meta name="author" content="Zhengxiang Wang, Jordan Kodner, Owen Rambow">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Zhengxiang Wang">
  <meta property="og:title" content="Evaluating LLMs with Multiple Problems at once">
  <meta property="og:description" content="Comprehensive multi-problem evaluation of LLMs under zero-shot settings.">
  <meta property="og:url" content="https://www.zhengxiang-wang.me/papers/multi-problem-eval-llm/">
  <meta property="og:image" content="https://www.zhengxiang-wang.me/images/research/multi_problem_eval.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Evaluating LLMs with Multiple Problems at once">
  <meta name="twitter:description" content="Comprehensive multi-problem evaluation of LLMs under zero-shot settings.">
  <meta name="twitter:image" content="https://www.zhengxiang-wang.me/images/research/multi_problem_eval.png">

  <meta name="citation_title" content="Evaluating LLMs with Multiple Problems at once">
  <meta name="citation_author" content="Wang, Zhengxiang">
  <meta name="citation_author" content="Kodner, Jordan">
  <meta name="citation_author" content="Rambow, Owen">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="GEM 2025">
  <meta name="citation_pdf_url" content="https://aclanthology.org/2025.gem-1.14.pdf">

  <title>Evaluating LLMs with Multiple Problems at once</title>

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
</head>
<body>
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="/papers/time-puzzles/" class="work-item">
          <div class="work-info">
            <h5>Measuring Iterative Temporal Reasoning with Time Puzzles</h5>
            <p>Constraint-based date inference benchmark for iterative temporal reasoning.</p>
            <span class="work-venue">Preprint 2026</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/lvlms-overhearing/" class="work-item">
          <div class="work-info">
            <h5>LVLMs are Bad at Overhearing Human Referential Communication</h5>
            <p>Evaluation of LVLM limitations in overhearing settings.</p>
            <span class="work-venue">EMNLP 2025 Main</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/implicit-writing-styles/" class="work-item">
          <div class="work-info">
            <h5>Catch Me If You Can? Not Yet</h5>
            <p>LLMs still struggle to imitate implicit writing styles of everyday authors.</p>
            <span class="work-venue">EMNLP 2025 Findings</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/multi-dimensional-writing-assessments/" class="work-item">
          <div class="work-info">
            <h5>LLMs can Perform Multi-Dimensional Analytic Writing Assessments</h5>
            <p>A case study of L2 graduate-level academic English writing.</p>
            <span class="work-venue">ACL 2025 Main</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/clustering-document-parts/" class="work-item">
          <div class="work-info">
            <h5>Clustering Document Parts</h5>
            <p>Detecting and characterizing influence campaigns from document collections.</p>
            <span class="work-venue">NLP+CSS 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="/papers/rnn-seq2seq-learning/" class="work-item">
          <div class="work-info">
            <h5>Learning Transductions and Alignments with RNN Seq2seq Models</h5>
            <p>Controlled study of transduction learning and generalization limits.</p>
            <span class="work-venue">ICGI 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>

      </div>
    </div>
  </div>

  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">Evaluating LLMs with Multiple Problems at once</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block"><a href="https://www.zhengxiang-wang.me/" target="_blank" rel="noopener">Zhengxiang Wang</a>,</span>
                <span class="author-block"><a href="https://jordankodner.github.io/" target="_blank" rel="noopener">Jordan Kodner</a>,</span>
                <span class="author-block"><a href="https://owenrambow.com/" target="_blank" rel="noopener">Owen Rambow</a></span>
              </div>
              <div class="is-size-5 publication-authors">
                <span class="author-block">GEM 2025</span>
              </div>
              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://aclanthology.org/2025.gem-1.14.pdf" target="_blank" rel="noopener"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fas fa-file-pdf"></i></span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/jaaack-wang/multi-problem-eval-llm" target="_blank" rel="noopener"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fab fa-github"></i></span>
                      <span>Code</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                We present a comprehensive multiple-problem evaluation (MPE) of large language models under
                zero-shot settings. Across 7 LLMs from 4 model families, we test both classification and
                reasoning prompts that contain multiple problems at once. The benchmark suite, ZeMPE,
                includes 53,100 zero-shot prompts spanning 6 classification and 12 reasoning benchmarks.
                Results show that LLMs can often handle multiple classifications in one prompt with only
                small degradation and better token efficiency, but performance drops consistently when task
                format changes (e.g., index-selection formulations) and when reasoning problems are mixed
                from different sources.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3 has-text-centered">Main Results</h2>
          <div class="results-carousel">
            <div class="item">
              <div class="content has-text-justified">
                <p>
                  For classification tasks, LLMs are generally robust to solving many items in one prompt.
                  Aggregated over models and benchmarks, SingleClf reaches 75.5% while BatchClf reaches
                  72.3% (3.2-point drop). However, reformulating the same underlying problems as selection
                  tasks causes large, consistent declines: SelectOne trails BatchClf by about 32 points and
                  SelectAll by about 10 points.
                </p>
              </div>
              <img src="static/images/main_results.png" alt="Average accuracy across LLMs, benchmarks, and task sizes for multiple-problem evaluation tasks." loading="lazy">
              <h2 class="subtitle has-text-centered">Average accuracy of 7 LLMs across four classification-related tasks and task sizes in MPE.</h2>
            </div>
            <div class="item">
              <div class="content has-text-justified">
                <p>
                  Multi-problem prompting can substantially reduce inference costs. At operating points that
                  preserve at least 95% of single-problem accuracy, reported token-cost savings range from
                  30.7% to 82.0% across model-benchmark pairs. This efficiency trend is visible in the
                  lower cost/accuracy ratio as task size grows, with only a few long-context outliers.
                </p>
              </div>
              <img src="static/images/cost_accuracy_ratio.png" alt="Cost-accuracy ratio comparison between single-problem and multi-problem prompting across models and benchmarks." loading="lazy">
              <h2 class="subtitle has-text-centered">Cost/accuracy ratio (lower is better) for SingleClf versus BatchClf across six benchmarks.</h2>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Conclusion</h2>
            <div class="content has-text-justified">
              <p>
                LLMs show strong innate capacity for zero-shot multiple-problem handling in standard
                batch classification and single-source reasoning, but this capability is brittle under
                format shifts and mixed-source reasoning prompts. The study introduces ZeMPE as a broad,
                reproducible benchmark for multi-problem evaluation and highlights a practical tradeoff:
                large cost savings are achievable, yet robust problem understanding across reformulations
                remains a central limitation.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@inproceedings{wang-etal-2025-evaluating,
    title = "Evaluating {LLM}s with Multiple Problems at once",
    author = "Wang, Zhengxiang  and
      Kodner, Jordan  and
      Rambow, Owen",
    editor = "Arviv, Ofir  and
      Clinciu, Miruna  and
      Dhole, Kaustubh  and
      Dror, Rotem  and
      Gehrmann, Sebastian  and
      Habba, Eliya  and
      Itzhak, Itay  and
      Mille, Simon  and
      Perlitz, Yotam  and
      Santus, Enrico  and
      Sedoc, Jo{\~a}o  and
      Shmueli Scheuer, Michal  and
      Stanovsky, Gabriel  and
      Tafjord, Oyvind",
    booktitle = "Proceedings of the Fourth Workshop on Generation, Evaluation and Metrics (GEM{\texttwosuperior})",
    month = jul,
    year = "2025",
    address = "Vienna, Austria and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.gem-1.14/",
    pages = "178--199",
    ISBN = "979-8-89176-261-9"
}</code></pre>
      </div>
    </section>
  </main>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank" rel="noopener">Academic Project Page Template</a>,
              adopted from the <a href="https://nerfies.github.io" target="_blank" rel="noopener">Nerfies</a> project page.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>
</html>
